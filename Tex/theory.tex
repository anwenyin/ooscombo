\section{Econometric Theory}
\subsection{Model and Estimation}
The econometric model to forecast is closely related to Hansen \cite{hansen2009averaging} and Andrews \cite{andrews93}. The model we are interested in is a linear time series regression with a possible structural break. The observations we have are $\{y_t,x_t\}$ for $t = 1,...T$, where $y_t$\footnote{Since we are interested in forecasting, $y_t$ can be thought of as the variable to be predicted for the next period using currently available information $x_t$.} is the scalar dependent variable and $x_t$ is a $k\times 1$ vector of related predictors and possibly lagged values of $y_t$. Parameters are estimated by least squares. The forecasting model\footnote{We assume that all information relevant for forecasting is included in the regressors $x_t$, and the source of model misspecification comes from the uncertainty about the parameter change. This is in contrast to most applied standard models introduced in textbooks where model misspecification bias comes from the wrong choice of regressors, but the parameters are assumed constant.} is:
\begin{equation}
	y_t = x_t'\beta_1 I_{[t<m]} + x_t'\beta_2 I_{[t \geq m]} + e_t
\end{equation}
where $I_{[\bullet]}$ is an indicator function and $E(e_t|x_t) = 0$. We will show two results in the subsequent analysis, depending on whether or not the error term is conditionally heteroscedastic. The break date is restricted to the interval $[m_1,m_2]$ which is bounded away from the ends of the sample on both sides, $1 < m_{1} < m_{2} < T$.

If the structural break is not detected\footnote{This can be done in two ways. The first is to treat various possible number of breaks as different models, then select one among these candidates by some information criterion, e.g., AIC, SIC or Mallow's. The other way is by hypothesis testing, following the relevant testing procedures outlined in Andrews\cite{andrews93}, Bai and Perron\cite{bai_perron98} and Elliot and Muller\cite{elliott_muller_RES2006}. }, we simply have a stable linear model:
\begin{equation}
	y_t = x_t'\beta + e_t
\end{equation}
We can then perform a test for structural breaks, either by using Andrews' SupF or SupW test or Bai and Perron's multiple break test, and then decide to keep the stable or unstable model. This is the pre-test procedure outlined in Hansen \cite{hansen2009averaging}. Under the assumption of conditional homoscedasticity, the test statistic is of SupF type. If we relax this assumption, the appropriate test statistic should be of Sup-Wald type since the F test does not allow for heteroscedastic errors. We use $\pi = m/T$ to denote the break fraction, where $m$ is the time index of the break date in the sample. Under the assumptions listed in Andrews \cite{andrews93} and under the null of no break, the SupF or SupW test statistic converges in distribution to the supremum of the square of a standardized tied-down Bessel process of order $k$, where $k \geq 1$ is the number of regressors in the model.\footnote{This distribution is non-standard so its critical values are obtained through simulation. Readers, especially empirical researchers, should be aware of the fact that critical values in Andrews' 1993 paper are incorrect and should not be used to conduct inference. Andrews' 2003 paper,\emph{Tests for Parameter Instability and Structural Change with Unknown Change Point: A Corrigendum}, corrects these values. Hansen \cite{hansen2009averaging} has done the same simulation in order to calculate his $\bar{p}$ value for model averaging weights. The author has replicated Andrews' critical value simulation during the writing of this paper and has obtained the same numerical results as Hansen but still different from Andrews' 2003 values. The difference may be caused by using different number of repetition in the simulation.}.

There are several popular information criteria for model selection: for example, Akaike information criterion (\textbf{AIC}), corrected AIC (\textbf{AIC\textsuperscript{c}}), Schwarz Bayesian information criterion (\textbf{SIC}), Hannan-Quinn (\textbf{HQ}) and Mallows criterion (\textbf{C\textsubscript{p}}). Relevant formulas are listed in table \ref{tb:1}.
\begin{table}
\centering
\caption{Information Criterion} \label{tb:1}
\begin{threeparttable}
\begin{tabular}{ll}
\toprule
AIC                 & $\mathrm{AIC}(k) = \log{(\tilde{\sigma}^{2}_{k})} + \frac{2k}{T}$ \\[0.4em]
$\mathrm{AIC}^{c}$  & $\mathrm{AIC}^{c}(k) = \mathrm{AIC}(k) + \frac{2k(k+1)}{T-k-1}$ \\[0.4em]
SIC                 & $\mathrm{SIC}(k) = \log{(\tilde{\sigma}^{2}_{k})} + \frac{k\log{(T)}}{T}$ \\[0.4em]
HQ                  & $\mathrm{HQ}(k) = \log{(\tilde{\sigma}^{2}_{k})} + \frac{2k\log{\log(T)}}{T}$ \\[0.4em]
Mallows            & $\mathrm{C}_{p}(k) = \hat{\sigma}^{2}_{k} + \frac{2k\tilde{\sigma}^{2}_{k}}{T}$\\
\bottomrule
\end{tabular}
\begin{tablenotes}[para, flushleft] \footnotesize
Notes: We use the number of regressors $k$ to index model. If there is a full structural break, the model index is $2k$.
\end{tablenotes}
\end{threeparttable}
\end{table}

Cross-validation is computationally simple for one-step ahead forecast models selection and robust to heteroscedasticity. Researchers have derived cross-validation weights for stable models, which can be translated into a quadratic programming problem. Hansen and Racine \cite{hansen2011jackknife} argue that for heteroscedastic forecasts, CV is a valid estimate of the one-step ahead MSFE for a combination forecast. They show that the CV weights are asymptotically optimal for cross-section data under heteroscedasticity.

The leave-one-out cross-validation criterion can be computed by the following procedure:
\begin{equation}
	CV_T(k) = \frac{1}{T}\sum_{t=1}^{T}\tilde{e}_{t}(k)^{2}
\end{equation}
where $\tilde{e}_{t}(k) = y_t - \tilde{\beta}_{-t}(k)'x_t(k)$ and $\tilde{\beta}_{-t}(k) = (\sum_{i\not= t}x_i(k) x_i(k)')^{-1}(\sum_{i\not= t}x_i(k) y_i)$. That is, at each time $t$, the parameters are estimated by omitting current observation $t$ and then computing its error. It may look tedious since we need to run $T$ regressions, but next section shows that this can be achieved efficiently with running a single regression. The cross-validation criterion is asymptotically an unbiased estimator of MSFE and is more robust to heteroscedasticity compared with Mallows' criterion. This is especially important when we are working with macroeconomic and financial time series data.

Since we only have two candidate models in this study, namely the stable model and the break model, to implement model averaging, we assign weight $w \in [0,1]$ to the break model and $1 - w$ to the stable model. The weights are those minimizing the CV criterion.
\subsection{Theoretical Results}
We assume that the true data generating process for $y_t$ takes the general parameter variation form and the change is of small magnitude so that the asymptotic distributions are asymptotically continuous. Under this assumption we can tell that the empirical model outlined in the previous section could be potentially misspecified: the single break model is only a special case of the general parameter variation model, with the absolute change of parameter values positive in one period while zero in others. In almost all relevant empirical work, researchers are interested in finding the best approximating model among all candidates available instead of looking for and estimating the true data generating process\footnote{There is research focusing on fitting data with complex models, such as nonlinear continuous break models. Though this approach may achieve good in-sample fit, its forecasting performance generally does not do well. As the simulation results of this paper show, when the true data generating process has continuous variation in its parameters, the simple no-break linear model forecasts better in terms of smaller MSFE than most complex methods.}. Our method gives better MSFE under model averaging forecast for possibly misspecified models than other methods.
\begin{Assumption}\label{asump:1}
Assume the following holds:
\begin{enumerate}
	\item The data generating process satisfies the linear model $y_t = x_t'\beta_t + e_t$, $t=1,...,T,\beta_t \in \mathbb{R}^k$, where $\beta_t = \beta + T^{-1/2}\eta(t/T)\delta\sigma_t$. $\eta(\bullet)$ is a $\mathbb{R}^k$ valued Riemann integrable function on $[0,1]$ and $\delta \in \mathbb{R}$ is a scalar indexing the magnitude of parameter variation.
	\item $\{(x_t',e_t)\}$ is $\alpha$-mixing of size $-r/(r-2),r > 2$ or $\phi$-mixing of size $-r/(2r-2),r \geq 2$.
    \item $E(x_t e_t) = 0, \forall t$, and the empirical process $\{x_t e_t\}$ is uniformly $L_r$-bounded, i.e., $||x_t e_t||_{r} < B$, where $B < \infty$.
    \item $T^{-1/2}\sum_{t=1}^{[\pi T]} x_t e_t \Rightarrow W(\pi)$ where $W(\pi)$ is a $k \times 1$ Wiener process with symmetric, positive definite covariance matrix $\Sigma \equiv \lim\limits_{T\to \infty}\mathrm{VAR}(T^{-1/2}\sum_{t=1}^{[\pi T]} x_t e_t)$, for $0 \leq \pi \leq 1$. ` $\Rightarrow$' denotes the weak convergence of the underlying probability measure as $T \to \infty$.
	\item $T^{-1}\sum_{t=1}^{[\pi T]}x_t x_t'$ converges uniformly to $\pi Q$ for all $\pi \in [0,1]$, $Q = E(x_t x_t')$ and all eigenvalues of $Q$ are uniformly bounded away from zero. $[\pi T]$ denotes the integer part of the product $\pi T$.
	\item $E(e_t^{2}|x_t) = \sigma_t^{2}$ or $E(e_t^{2}|x_t) = \sigma^{2}$.
\end{enumerate}
\end{Assumption}
Assumption 1.1 says that the true data generating process for $y_t$ takes a general parameter variation form and structural break occurs in all parameters. This assumption follows the theory proposed by Andrews and Hansen but is different from Bai and Perron who consider partial structural breaks. This data generating process includes several cases that have been studied by researchers, such as a single break or continuous break. Notice that our forecast model only allow for one possible break in the parameters, so it could potentially be misspecified. We make this assumption and allow for the gap between DGP and model primarily for two reasons. First, as argued by several researchers, for example, L\"{u}tkepohl \cite{lutkepohl_textbook}, the true data generating process for many macroeconomic and financial variables may be a complicated process possibly involving past values of infinite order. The main focus is not to search for the true DGP but to find the best approximating model based on available information to forecast. This assumption tries to capture the difficulty faced by many practitioners in forecasting. Second, a simple model is more likely to forecast better than many complex methods. As shown in one case of the simulation section, even though the true DGP is a continuous break process, the stable model forecasts better than most weighting methods.

Assumption 1.2--1.5 ensures that we can apply all relevant mixing law of large numbers, central limit theorem and Donsker's invariance principle. Assumption 1.6 says that the error term could be conditionally homoscedastic or heteroscedastic, depending on the application of interest.

We start by providing a lemma showing that the CV residuals can be efficiently computed by using the leverage of each observation without actually having to run least squares regression $T$ times.
\begin{lemma} \label{lem:1}
Leave--one--out cross--validation estimation residuals can be computed from full sample least squares residuals, $\tilde{e_t} = \frac{\hat{e_t}}{1-h_t}$, where $h_t = x_t' (X'X)^{-1} x_t$ is the leverage.
\end{lemma}

The main theoretical results in this paper are to derive the asymptotic distribution of the penalty term in the CV criterion since its first moment is crucial in determining the optimal weights in model averaging. We discuss two possible cases depending on whether the assumption of conditional homoskedasticity holds. The proofs of all theoretical results are provided in the appendix.
\begin{theorem} \label{thm:1}
If assumption~\ref{asump:1} holds under conditional homoskedastic errors, the leave-one-out cross--validation criterion is approximately equivalent to Mallows' criterion. Specifically, the weight for the break model is
\begin{equation}
\hat{w} = \frac{(T - 2k)(\sum_{t=1}^{T}\tilde{e}_{t}^{2} - \sum_{t=1}^{T}\hat{e}_{t}^{2}) - \bar{p}\sum_{t=1}^{T}\hat{e}_{t}^{2}}{(T - 2k)(\sum_{t=1}^{T}\tilde{e}_{t}^{2} - \sum_{t=1}^{T}\hat{e}_{t}^{2})}
\end{equation}
where $T$ is the sample size, $k$ is the number of regressors, $\hat{e}_t$s are the OLS residuals from the break model, $\tilde{e}_t$s are residuals from the stable model, $\bar{p}$ is the penalty coefficient whose value depends on the asymptotic distribution of the SupW test statistic.
\end{theorem}
This theorem shows that if we assume conditional homoscedasticity in the model, then the CV criterion is approximately identical to the Mallows' criterion\footnote{This is quite intuitive because the major advantage of CV over Mallows is that CV is robust to heteroscedasticity. If we assume away of conditional heteroscedasticity, then these two criteria are almost equivalent.}, so all the model averaging results in Hansen \cite{hansen2009averaging}, i.e., optimal weights, hold under cross--validation.

It is widely known in the model selection literature that the CV criterion is superior to Mallows' and other information criteria because of its robustness to heteroscedasticity \cite{andrews_JE1991}. Our next theorem shows the asymptotic distribution of the CV penalty term if we relax the assumption of conditional homoscedasticity.
\begin{theorem} \label{thm:2}
If assumption~\ref{asump:1} holds under conditionally heteroscedastic errors, the penalty term in the CV criterion converges in distribution to a weighted sum of independent $\chi^2$ distribution with degree of freedom one plus a distribution which is a function of non-standard Brownian bridge,
\begin{equation} \label{eq:4}
	e'P(\hat{k})e \stackrel{d}{\rightarrow} \sum_{j=1}^{k} \lambda_j \chi^2(1) + J_0(\xi_{\delta})
\end{equation}
where $\lambda_j$s are the eigenvalues of the matrix $L'Q^{-1}L$, $L$ comes from the decomposition of the positive definite matrix $\Sigma = LL'$, $\Sigma$ is the long-run variance of $\rn\jian X_t e_t$, $Q = E(x_t x_t')$ and $J_0(\xi_{\delta})$ is the asymptotic distribution of the Sup-Wald type statistic as in Andrews \cite{andrews93} under the true data generating process.
\end{theorem}
Now the asymptotic distribution of the penalty term involves a weighted sum of $\chi^2$ distributions and a Sup-Wald type distribution under the true data generating process. Comparing this result with the previous one, we can see that the distribution under conditional homoskedasticity is just a special case of the new distribution. That is, the weights on the $\chi^2$ random variables are identical and take the value of one, which results in a $\chi^2$ distribution with degrees of freedom equal to the number of regressors. For the second part in~(\ref{eq:4}), since Andrews \cite{andrews93} proves the asymptotic distribution for the Sup-Wald type statistic under quite general conditions, his results still hold here under heteroscedastic errors.

Notice that the new asymptotic distribution for the penalty term is non-standard, but its mean is relatively easy to compute, which is the only moment we care about in computing optimal model weights. The expectation of $\sum_{j=1}^{k} \lambda_j \chi^2(1)$ is simply $\sum_{j=1}^{k} \lambda_j$ which is the trace of the matrix $Q^{-1} \Sigma$, where $\Sigma$ is the long-run variance of $\rn\jian X_t e_t$ and $Q = E(x_t x_t')$. Empirically, $\Sigma$ can be estimated by HAC or HAR estimators and $Q$ can be consistently estimated by its sample analogue $\frac{1}{T}\sum_{t=1}^{T}x_t x_t'$. The following lemma says what the weight is for the break model with conditionally heteroscedastic errors.

\begin{lemma} \label{lem:2}
Suppose that $\hat{e}_t$s are the OLS residuals from the break model and $\tilde{e}_t$s are residuals from the stable model, then the optimal weight for the break model with conditional heteroscedasticity takes the form \footnote{$\mathrm{E(SupW(\pi_1,k))}$ is the expectation of the SupW distribution under the null of no break in Andrews' paper, evaluated at given values of $\pi$ and $k$. Note that the expectation of the SupW distribution under the true data generating process depends on unknown parameters, so Hansen suggests that in practice we can approximate its value by taking the averaging of two extreme cases: $\delta = 0$ and $\delta = \infty$. $\delta = 0$ implies that the distribution is identical to the SupW distribution under the null of no break. However, $\delta = \infty$ indicates that when the break size goes to infinity, essentially it is like we know exactly when the break happens, so its distribution becomes the $\chi^2$ type distribution whose first moment can be easily obtained without simulation.}:
\begin{equation}
	\hat{w} = 1 - \frac{\frac{1}{2}(\sum_{j=1}^{k}\lambda_j + \mathrm{E(SupW(\pi_1,k))})}{\sum_{t=1}^{T}\hat{e}_t^2 - \sum_{t=1}^{T}\tilde{e}_t^2}
\end{equation}
where the expectation of the SupW distribution depends on the number of regressors $k$ and the value of the trimming parameter, $\pi_1$\footnote{In practice, a popular choice of $\pi_1$ is $0.15$.}.
\end{lemma} 