\section{Econometric Theory}
\subsection{Model and Estimation}
The econometric model to forecast and its estimation method are closely related to Hansen \cite{hansen2009averaging} and Andrews\footnote{Andrews considers GMM as the primary estimation method.} \cite{andrews93}. The model we are interested in is a linear time series regression with a possible structural break. The observations we have are time series $\{y_t,x_t\}$ for $t = 1,...T$, where $y_t$\footnote{Since we are interested in forecasting, $y_t$ can be thought of as the variable to be predicted for the next period using currently available information $x_t$.} is the scalar dependent variable and $x_t$ is a $k\times 1$ vector of related predictors\footnote{It includes the constant term.} and possibly lagged values of $y_t$, $k$ is the number of regressors or predictors included. Parameters are estimated by least squares. The forecasting model\footnote{We assume that all information relevant for forecasting is included in the regressors $x_t$, and the source of model misspecification comes from the uncertainty about the parameter change. This is in contrast to most applied econometric models introduced in textbooks where model misspecification bias comes from the wrong choice of regressors, but the parameters are assumed constant.} allowing for structural break is:
\begin{equation}
	y_t = x_t'\beta_1 I_{[t<m]} + x_t'\beta_2 I_{[t \geq m]} + e_t
\end{equation}
where $I_{[\bullet]}$ is an indicator function and $E(e_t|x_t) = 0$. The break date is restricted to the interval $[m_1,m_2]$ which is bounded away from the ends of the sample on both sides, $1 < m_{1} < m_{2} < T$. In practice, a popular choice is to use the middle $70\%$ portion of the sample. If the break happens very close to the end of the sample, we may not be able to identify the break or the regression parameters.

We can also use a stable linear model to forecast:
\begin{equation}
	y_t = x_t'\beta + e_t
\end{equation}
Traditionally, we start the process by performing a test for structural breaks\footnote{This can be done in two ways. The first is to treat various possible number of breaks as different models, then select one according to some information criterion, e.g., AIC, SIC or Mallow's. The other way is hypothesis testing, following the relevant testing procedures outlined in Andrews\cite{andrews93}, Bai and Perron\cite{bai_perron98} and Elliot and Muller\cite{elliott_muller_RES2006}. }, either by using Andrews' SupF or SupW test, or Bai and Perron's multiple-break test, and then decide to keep the stable or unstable model. This is the pre-test procedure outlined in Hansen \cite{hansen2009averaging}. Under the assumption of conditional homoscedasticity, the Andrews' test statistic is of SupF type. If we relax this assumption, the appropriate test statistic should be of Sup-Wald type since the F test does not allow for heteroscedastic errors. We use $\pi = m/T$ to denote the break fraction, where $m$ is the time index of the break date in the sample. Under the assumptions listed in Andrews \cite{andrews93} and under the null of no break, the SupF or SupW test statistic converges in distribution to the supremum of the square of a standardized tied-down Bessel process of order $k$, where $k \geq 1$ is the number of regressors in the model.\footnote{This distribution is non-standard so its critical values are obtained through simulation. Readers, especially applied economics researchers, should be aware of the fact that critical values in Andrews' 1993 paper are incorrect and should not be used to conduct inference. Andrews' 2003 paper,\emph{Tests for Parameter Instability and Structural Change with Unknown Change Point: A Corrigendum}, corrects these values. Hansen \cite{hansen2009averaging} has done the same simulation in order to calculate his $\bar{p}$ value for model averaging weights. The author has replicated Andrews' critical value simulation during the writing of this paper and has obtained the same numerical results as Hansen but they are still different from Andrews' 2003 values. The difference may be caused by using different number of repetition in the simulation.}.
\subsection{Cross-Validation Criterion}
There are several popular information criteria for model selection: for example, Akaike information criterion (\textbf{AIC}), corrected AIC (\textbf{AIC\textsuperscript{c}}), Schwarz Bayesian information criterion (\textbf{SIC}), Hannan-Quinn (\textbf{HQ}) and Mallows criterion (\textbf{C\textsubscript{p}}). Their formulas are listed in table \ref{tb:1}.
\begin{table}
\centering
\caption{Information Criterion} \label{tb:1}
\begin{threeparttable}
\begin{tabular}{ll}
\toprule
AIC                 & $\mathrm{AIC}(k) = \log{(\tilde{\sigma}^{2}_{k})} + \frac{2k}{T}$ \\[0.4em]
$\mathrm{AIC}^{c}$  & $\mathrm{AIC}^{c}(k) = \mathrm{AIC}(k) + \frac{2k(k+1)}{T-k-1}$ \\[0.4em]
SIC                 & $\mathrm{SIC}(k) = \log{(\tilde{\sigma}^{2}_{k})} + \frac{k\log{(T)}}{T}$ \\[0.4em]
HQ                  & $\mathrm{HQ}(k) = \log{(\tilde{\sigma}^{2}_{k})} + \frac{2k\log{\log(T)}}{T}$ \\[0.4em]
Mallows            & $\mathrm{C}_{p}(k) = \hat{\sigma}^{2}_{k} + \frac{2k\tilde{\sigma}^{2}_{k}}{T}$\\
\bottomrule
\end{tabular}
\begin{tablenotes}[para, flushleft] \footnotesize
Notes: We use the number of regressors $k$ to index model. If there is a full structural break, the model index is $2k$.
\end{tablenotes}
\end{threeparttable}
\end{table}

Cross-validation is computationally simple for one-step ahead forecasting model selection and is robust to conditional heteroscedasticity. Researchers have derived cross-validation weights for averaging stable models, which can be translated into a quadratic programming problem. Hansen and Racine \cite{hansen2011jackknife} argue that for heteroscedastic data, cross-validation is a valid estimate of the one-step ahead MSFE. They show that the CV weights are asymptotically optimal for cross-section data under heteroscedasticity.

The leave-one-out cross-validation criterion can be computed by the following procedure:
\begin{equation}
	CV_T(k) = \frac{1}{T}\sum_{t=1}^{T}\tilde{e}_{t}(k)^{2}
\end{equation}
where $\tilde{e}_{t}(k) = y_t - \tilde{\beta}_{-t}(k)'x_t(k)$ and $\tilde{\beta}_{-t}(k) = (\sum_{i\not= t}x_i(k) x_i(k)')^{-1}(\sum_{i\not= t}x_i(k) y_i)$. That is, at each time period $t$, the parameters are estimated by using all data available but omitting current observation $t$, and then computing its error. It may look tedious since we need to run $T$ times of regression to get the value of CV, but in next section we show that this can be achieved efficiently by running regression only once. The cross-validation criterion is asymptotically an unbiased estimator of MSFE and is robust to heteroscedasticity compared with Mallows' criterion. This is especially important when we are working with macroeconomic and financial time series.

Since we only have two candidate models in this study, namely the stable model and the break model, to implement model averaging, we assign weight $w \in [0,1]$ to the break model and $1 - w$ to the stable model. The optimal weights are those minimizing the CV criterion.
\subsection{Theoretical Results}
We assume that the true data generating process for $y_t$ takes the general parameter variation form and the change is of small magnitude so that the asymptotic distributions are asymptotically continuous. Also the change of the parameter value is proportional to the unconditional standard deviation of the error term. This makes sure that the impact of the local parameter instability is not dominated by that of the volatility. Under this assumption we can tell that the empirical model outlined in the previous section could be potentially misspecified: the single break model is only a special case of the general parameter variation model, with the absolute change of parameter values positive in one period while zero in others. In almost all relevant empirical work, researchers are interested in finding the best approximating model among all candidates available instead of looking for and estimating the true data generating process\footnote{There is research focusing on fitting data with complex models, such as nonlinear continuous break models. Though this approach may achieve good in-sample fit, its forecasting performance generally does not do well.}. Based on assumptions listed below, our method should give better MSFE under model averaging forecast for possibly misspecified models than other related methods.
\begin{Assumption}\label{asump:1}
Assume the following holds:
\begin{enumerate}
	\item The true data generating process satisfies the linear model $y_t = x_t'\beta_t + e_t$, $t=1,...,T,\beta_t \in \mathbb{R}^k$, where $\beta_t = \beta + T^{-1/2}\eta(t/T)\delta\sigma_t$. $\eta(\bullet)$ is a $\mathbb{R}^k$ valued Riemann integrable function on $[0,1]$ and $\delta \in \mathbb{R}$ is a scalar indexing the magnitude of parameter variation.
	\item $\{(x_t',e_t)\}$ is $\alpha$-mixing of size $-r/(r-2),r > 2$ or $\phi$-mixing of size $-r/(2r-2),r \geq 2$.
    \item $E(x_t e_t) = 0, \forall t$, and the empirical process $\{x_t e_t\}$ is uniformly $L_r$-bounded, i.e., $||x_t e_t||_{r} < B$, where $B$ is a constant and $B < \infty$.
    \item $T^{-1/2}\sum_{t=1}^{[\pi T]} x_t e_t \Rightarrow W(\pi)$ where $W(\pi)$ is a $k \times 1$ Wiener process with symmetric, positive definite long-run covariance matrix $\Sigma \equiv \lim\limits_{T\to \infty}\mathrm{VAR}(T^{-1/2}\sum_{t=1}^{[\pi T]} x_t e_t)$, for $0 \leq \pi \leq 1$. ` $\Rightarrow$' denotes the weak convergence of the underlying probability measure as $T \to \infty$.
	\item $T^{-1}\sum_{t=1}^{[\pi T]}x_t x_t'$ converges uniformly to $\pi Q$ for all $\pi \in [0,1]$, $Q = E(x_t x_t')$ and all eigenvalues of $Q$ are uniformly bounded away from zero. $[\pi T]$ denotes the integer part of the product $\pi T$.
	\item $E(e_t^{2}|x_t) = \sigma_t^{2}$ or $E(e_t^{2}|x_t) = \sigma^{2}$.
\end{enumerate}
\end{Assumption}
Assumption 1.1 says that the true data generating process for $y_t$ takes a general parameter variation form and structural break occurs in all parameters. This assumption follows the theory proposed by Andrews and Hansen but is different from Bai and Perron who consider partial structural breaks. For Bai and Perron's \cite{bai_perron98} partial structural break model, there is no clear guidance on how to determine which set of regressors are subject to parameter instability. Without any prior information on the stability of parameters, it is natural to assume that all coefficients are subject to break. This full-break assumption is less restrictive and is used in many empirical applications, see Rapach and Wohar \cite{rapach_wohar_JFE2006} and Paye and Timmermann \cite{paye_timmermann_JEF2006}. This type of data generating process includes several commonly used cases, such as a single break, multiple breaks or continuous break process. Notice that our forecasting model outlined in the previous section only allows for one possible break in the parameters, so it could potentially be misspecified. We make this assumption and allow for the gap between the true DGP and the forecasting model primarily for two reasons. First, as argued by several researchers, for example, L\"{u}tkepohl \cite{lutkepohl_textbook}, the true data generating process for many macroeconomic and financial variables may be a complicated process possibly involving past values of infinite order. The main focus is not to search for the true DGP but to find the best approximating model, or the best local linear predictor based on available information. This assumption tries to capture the difficulty faced by many practitioners in forecasting. Second, a simple model is more likely to forecast better than many complex methods on average. Complicated methods may achieve good in-sample fit at the expense of compromising the forecasting model's adaptability to environment change\footnote{In an earlier version of this paper, through Monte Carlo simulation we show that even the true DGP follows a continuous break process, the stable model forecasts best among all popular methods considered, such as Granger-Ramanathan combination.}.

Assumption 1.2 -- 1.5 ensure that we can apply all relevant mixing law of large numbers, functional central limit theorem or Donsker's invariance principle to proving our results. See Davidson \cite{davidson_textbook} for more details on advanced asymptotic theory. Assumption 1.6 says that the error term could be conditionally homoscedastic or heteroscedastic, depending on the application of interest. This assumption contrasts our paper with Hansen's \cite{hansen2009averaging}, as Hansen only considers the conditionally homoscedastic error case.

We start by providing a lemma showing that the cross-validation residuals can be efficiently computed by using the leverage of each observation without actually having to run least squares regression $T$ times.
\begin{lemma} \label{lem:1}
The leave--one--out cross--validation estimation residuals can be computed from the full sample least squares residuals, $\tilde{e_t} = \frac{\hat{e_t}}{1-h_t}$, where $h_t = x_t' (X'X)^{-1} x_t$ is the leverage, $\hat{e_t}$ is the least squares residual and $\tilde{e_t}$ is the cross-validation residual.
\end{lemma}

Next, to obtain the main theoretical results, we need to derive the asymptotic distribution of the penalty term in the CV criterion since its first moment is crucial in determining the optimal weights in model averaging. We provide two theorems for the model averaging weights. Their differences depend on whether the assumption of conditional homoscedasticity holds. The proofs of all theoretical results are provided in the appendix.
\begin{theorem} \label{thm:1}
If assumption~\ref{asump:1} holds under conditional homoscedastic errors, the leave-one-out cross--validation criterion is approximately equivalent to Mallows' criterion, so CV weights are equivalent to Mallows' weights. Specifically, the weight for the break model is
\begin{equation}
\hat{w} = \frac{(T - 2k)(\sum_{t=1}^{T}\tilde{e}_{t}^{2} - \sum_{t=1}^{T}\hat{e}_{t}^{2}) - \bar{p}\sum_{t=1}^{T}\hat{e}_{t}^{2}}{(T - 2k)(\sum_{t=1}^{T}\tilde{e}_{t}^{2} - \sum_{t=1}^{T}\hat{e}_{t}^{2})}
\end{equation}
where $T$ is the sample size, $k$ is the number of regressors, $\hat{e}_t$s are the OLS residuals from the break model, $\tilde{e}_t$s are residuals from the stable model, $\bar{p}$ is the penalty coefficient whose value depends on the asymptotic distribution of the SupW test statistic.
\end{theorem}
This theorem shows that if we assume conditional homoscedasticity in the model, then the cross-validation information criterion is approximately identical to the Mallows' criterion\footnote{This is quite intuitive because the major advantage of CV over Mallows is that CV is robust to heteroscedasticity. If we assume away conditional heteroscedasticity, then these two criteria are basically equivalent.}, so all the model averaging results in Hansen \cite{hansen2009averaging}, i.e., optimal weights, hold here. See Hansen \cite{hansen2009averaging} for the table for $\bar{p}$ values.

It is widely known in the model selection literature that the CV criterion is superior to Mallows' and other information criteria because of its robustness to heteroscedasticity \cite{andrews_JE1991}. Our next lemma shows the asymptotic distribution of the CV penalty term if we relax the assumption of conditional homoscedasticity.
\begin{lemma} \label{thm:2}
If assumption~\ref{asump:1} holds under conditionally heteroscedastic errors, the penalty term in the cross-validation criterion converges in distribution to a weighted sum of independent $\chi^2$ distribution with degree of freedom one plus a term whose distribution is a function of non-standard Brownian bridge,
\begin{equation} \label{eq:4}
	e'P(\hat{k})e \stackrel{d}{\rightarrow} \sum_{j=1}^{k} \lambda_j \chi^2(1) + J_0(\xi_{\delta})
\end{equation}
where $\lambda_j$s are the eigenvalues of the matrix $L'Q^{-1}L$, $L$ comes from the decomposition of the positive definite matrix $\Sigma = LL'$, $\Sigma$ is the long-run variance of $\rn\jian X_t e_t$, $Q = E(x_t x_t')$ and $J_0(\xi_{\delta})$ is the asymptotic distribution of the Sup-Wald type statistic as in Andrews \cite{andrews93} under the true data generating process.
\end{lemma}
Now the asymptotic distribution of the penalty term involves a weighted sum of $\chi^2$ distributions and a Sup-Wald type distribution under the true data generating process. Comparing this result with the previous one, we can see that the distribution under conditional homoscedasticity is just a special case of the new one. That is, the weights for the $\chi^2$ random variables are identical and they take the value of one, which results in a $\chi^2$ distribution with degrees of freedom equal to the total number of regressors. 

Notice that the new asymptotic distribution of the penalty term is non-standard, but its expectation is relatively easy to compute, which is the only moment we care about in computing the optimal model averaging weights. The expectation of $\sum_{j=1}^{k} \lambda_j \chi^2(1)$ is simply $\sum_{j=1}^{k} \lambda_j$ which is the trace of the matrix $Q^{-1} \Sigma$, where $\Sigma$ is the long-run variance of $\rn\jian X_t e_t$ and $Q = E(x_t x_t')$. Empirically, $\Sigma$ can be estimated by HAC or HAR estimators and $Q$ can be consistently estimated by its sample analogue $\frac{1}{T}\sum_{t=1}^{T}x_t x_t'$. The following theorem provides the optimal weight for the break model under conditionally heteroscedasticity.

\begin{theorem} \label{lem:2}
Suppose that $\hat{e}_t$s are the OLS residuals from the break model and $\tilde{e}_t$s are residuals from the stable model, then the optimal weight for the break model under conditional heteroscedasticity takes the form \footnote{$\mathrm{E(SupW(\pi_1,k))}$ is the expectation of the SupW distribution under the null of no break in Andrews' paper, evaluated at given values of $\pi$ and $k$. Note that the expectation of the SupW distribution under the true data generating process depends on unknown parameters, so Hansen suggests that in practice we can approximate its value by taking the averaging of two extreme cases: $\delta = 0$ and $\delta = \infty$. $\delta = 0$ implies that the distribution is identical to the SupW distribution under the null of no break. However, $\delta = \infty$ indicates that when the break size goes to infinity, essentially it is like we know exactly when the break happens, so its distribution becomes the $\chi^2$ type distribution whose first moment can be easily obtained without simulation.}:
\begin{equation} \label{eq:5}
	\hat{w} = 1 - \frac{\frac{1}{2}(\sum_{j=1}^{k}\lambda_j + \mathrm{E(SupW(\pi_1,k))})}{\sum_{t=1}^{T}\hat{e}_t^2 - \sum_{t=1}^{T}\tilde{e}_t^2}
\end{equation}
where the expectation of the SupW distribution depends on the number of regressors $k$ and the value of the trimming parameter, $\pi_1$\footnote{In practice, a popular choice of $\pi_1$ is $0.15$.}.
\end{theorem}
Note that the value of $\mathrm{E(SupW(\pi_1,k))}$ can be obtained from the $\bar{p}$ value\footnote{$\bar{p} = \frac{1}{2}(\mathrm{E(SupW(\pi_1,k))} + k)$} in Hansen \cite{hansen2009averaging}. The important part is the first component $\sum_{j=1}^{k}\lambda_j$ in the parentheses of equation~\ref{eq:5}. Intuitively, it captures the impact brought to the weight by allowing for conditional heteroscedasticity. Theorem~\ref{lem:2} implies that when there is conditional heteroscedasticity, cross-validation weights are optimal in the sense of minimizing the mean squared forecast error.