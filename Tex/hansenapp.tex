\section{Empirical Application}
Here we apply our methods to forecast U.S. GDP growth rate\footnote{The data are available from Hansen's website:\url{http://www.ssc.wisc.edu/~bhansen/cbc/}.} out-of-sample and compare their performance with others\footnote{The reason we choose this dataset to examine CV and Cp forecast performance is because various researchers have used either stable or break models to forecast GDP growth rate. Additionally, various heteroscedasticity-robust estimation methods are used by researchers. To our knowledge, we have not seen any forecast combination methods explicitly combining the break and stable model to forecast U.S. GDP, though some combination methods may implicitly include forecasts generated by break models.}. We have quarterly data from 1960:1 to 2012:1, 209 total observations. The variable we are interested in forecasting is the U.S. quarterly GDP growth rate. Predictors included in some models are the quarterly change of U.S. 3-month treasury rate ($\Delta\mathrm{SR}$), the quarterly change of U.S. 10-year treasury rate ($\Delta\mathrm{LR}$) and the quarterly change of default premium ($\Delta\mathrm{DP}$)\footnote{In the database, these predictors are given in levels, but we have converted them to rates of change, so the actual sample size in all models except for the AR(1) is 207. The default premium is calculated by the difference between the AAA bond rate and BAA bond rate.}.

We consider five models, from small to large they are:
\begin{subequations}
\begin{align}
\Delta\mathrm{GDP}_{t} & = \beta_{0} + \beta_{1}\Delta\mathrm{GDP}_{t-1} + \epsilon_{t} \label{md:1}\\
\Delta\mathrm{GDP}_{t} & = \beta_{0} + \beta_{1}\Delta\mathrm{GDP}_{t-1} + \beta_{2}\Delta\mathrm{GDP}_{t-2} + \epsilon_{t}\label{md:2} \\
\Delta\mathrm{GDP}_{t} & = \beta_{0} + \beta_{1}\Delta\mathrm{GDP}_{t-1} + \beta_{2}\Delta\mathrm{SR}_{t-1} + \epsilon_{t}\label{md:3} \\
\Delta\mathrm{GDP}_{t} & = \beta_{0} + \beta_{1}\Delta\mathrm{GDP}_{t-1} + \beta_{2}\Delta\mathrm{SR}_{t-1} + \beta_{3}\Delta\mathrm{LR}_{t-1} + \epsilon_{t}\label{md:4} \\
\Delta\mathrm{GDP}_{t} & = \beta_{0} + \beta_{1}\Delta\mathrm{GDP}_{t-1} + \beta_{2}\Delta\mathrm{SR}_{t-1} + \beta_{3}\Delta\mathrm{LR}_{t-1} + \beta_{4}\Delta\mathrm{DP}_{t-1} + \epsilon_{t}\label{md:5}
\end{align}
\end{subequations}
Since in reality we do not know the ``true'' econometric model specification, we consider the above five candidate models. For each model, we examine the performance of various model averaging methods. Consistent with what is done in the previous simulation section, for each model we use the recursive window to forecast out-of-sample. To better examine the OOS performance, in each case we generate a sequence of RMSFE by varying the evaluation sample size, from 15 to 50, with increments of 5. Forecast results from all five models are presented in table~\ref{ntb:3}. For simplicity and ease of comparison, we again pick the equal weight method as the benchmark and normalize all OOS forecasting performance around $1$. If the value is below $1$, then the OOS forecasts perform better than that of the equal weight method.

We can see that in all five models approximating the DGP, CV forecasts better than SIC, Cp and equal weight under recursive window across all evaluation sample sizes. Additionally, CV is the only method which beats equal weight. The forecast gains of CV relative to equal weight range from about $1\%$ to $6\%$ across evaluation sample sizes and models. CV solves the forecast combination puzzle in this application.

\begin{table}
    \caption{OOS U.S. Quarterly GDP Growth Rate Forecast Comparison} \label{ntb:3}
    \centering
    \begin{adjustbox}{width=\textwidth,totalheight=\textheight,keepaspectratio}
    \begin{threeparttable}
    \begin{tabular}{lccccccccccccccc}
    \toprule
     & \multicolumn{3}{c}{Model a} & \multicolumn{3}{c}{Model b} & \multicolumn{3}{c}{Model c} & \multicolumn{3}{c}{Model d} & \multicolumn{3}{c}{Model e}\\%[0.3em]
    \cmidrule(r){2-4}
    \cmidrule(r){5-7}
    \cmidrule(r){8-10}
    \cmidrule(r){11-13}
    \cmidrule(r){14-16}\\
           & Cp    & CV    & SIC  & Cp    & CV    & SIC & Cp    & CV    & SIC & Cp    & CV    & SIC & Cp    & CV    & SIC \\
    P = 15 & 1.040 & 0.965 & 0.999& 1.043 & 0.995 &0.999& 1.004 & 0.998 &1.000& 1.027 & 0.976 &0.999& 1.041 & 0.964 &0.997 \\
    P = 20 & 1.044 & 0.967 & 0.999& 1.031 & 0.983 &0.999& 1.017 & 0.987 &0.999& 1.038 & 0.970 &0.998& 1.043 & 0.960 &0.997 \\
    P = 25 & 1.038 & 0.968 & 0.999& 1.021 & 0.984 &0.999& 1.036 & 0.976 &0.999& 1.038 & 0.969 &0.998& 1.017 & 0.967 &0.998 \\
    P = 30 & 1.022 & 0.977 & 0.999& 1.022 & 0.983 &0.999& 1.007 & 0.996 &1.000& 1.013 & 0.991 &0.998& 1.032 & 0.975 &0.998 \\
    P = 35 & 1.020 & 0.980 & 1.000& 1.036 & 0.996 &0.999& 1.022 & 0.983 &0.999& 1.024 & 0.983 &0.999& 1.034 & 0.973 &0.998 \\
    P = 40 & 1.022 & 0.979 & 0.999& 1.012 & 0.987 &1.000& 1.024 & 0.982 &0.999& 1.025 & 0.982 &0.999& 1.033 & 0.974 &0.998 \\
    P = 45 & 1.024 & 0.978 & 1.000& 1.014 & 0.986 &1.000& 1.025 & 0.982 &0.999& 1.026 & 0.981 &0.999& 1.037 & 0.974 &0.998 \\
    P = 50 & 1.021 & 0.987 & 1.000& 1.011 & 0.989 &1.000& 1.027 & 0.984 &0.999& 1.023 & 0.987 &0.999& 1.022 & 0.988 &0.999 \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}[para, flushleft] \footnotesize
    Notes: Quarterly data from 1960:1 to 2012:1. $\mathrm{P}$ is the evaluation sample size. Equal weight is chosen as the benchmark and the numbers in the table represent the RMSFE ratio between each individual method and equal weight. Smaller number indicates better forecasting performance. Cp: Mallows' weights. CV: cross-validation weights. SIC: Schwarz-Bayesian weights.
    \newline Model a: AR(1)
    \newline Model b: AR(2)
    \newline Model c: AR(1) + SR
    \newline Model d: AR(1) + SR + LR
    \newline Model e: AR(1) + SR + LR + DP
    \end{tablenotes}
    \end{threeparttable}
    \end{adjustbox}
    %\end{sidewaystable}
\end{table}
%Since in this section we are only interested in comparing our methods with others and in trying to somehow solve the forecast combination puzzle, we do not select one best model approximating %the DGP among those five candidates listed above. In practice, researcher may select a best   