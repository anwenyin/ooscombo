\subsection{Cross-Validation Criterion}
There are several popular information criteria for model selection: for example, Akaike information criterion (\textbf{AIC}), corrected AIC (\textbf{AIC\textsuperscript{c}}), Schwarz Bayesian information criterion (\textbf{SIC}), Hannan-Quinn (\textbf{HQ}) and Mallows' C\textsubscript{p} (\textbf{C\textsubscript{p}}). Most criteria have two components in their formulas: the first part measures model fit while the second penalizes overfitting. The quantity measuring in-sample fit are the same for most criteria, but they differ in the degree of penalization. For instance, AIC penalizes each additional parameter by $2$ while SIC penalizes overfitting by the logarithm of sample size, so SIC tends to select a more parsimonious model than AIC if the sample size is large.

For the forecasting analysis, what we care about is the test error rate assessing the model predictive ability, not the training error rate produced in the model estimation stage, so selecting a information criterion which gives a good estimate of the expected test error rate is crucial. Cross-validation is such a criterion. Specifically, we focus on the use of leave-one-out cross-validation for this paper, though other CV variants, such as K--fold cross-validation, may be considered. Cross-validation is computationally simple for one-step ahead predictive model selection and is shown robust to conditional heteroscedasticity in the econometrics and statistics literature. For forecast combination, researchers have applied CV to the quadratic programming based model averaging analysis, but its setting does not include structural break.

The sample leave-one-out cross-validation criterion can be computed by the following procedure:
\begin{equation} \label{cv}
	CV_T(k) = \frac{1}{T}\sum_{t=1}^{T}\tilde{e}_{t}(k)^{2}
\end{equation}
where $\tilde{e}_{t}(k) = y_t - \tilde{\beta}_{-t}(k)'x_t(k)$ are the residuals from the regression with the $t^{\mathrm{th}}$ observation dropped and $\tilde{\beta}_{-t}(k) = (\sum_{i\not= t}x_i(k) x_i(k)')^{-1}(\sum_{i\not= t}x_i(k) y_i)$ is the associated vector of parameter estimates. Intuitively, this procedure is trying to estimate the expected test error rate based on the model training data. Though equation~\ref{cv} implies that we need to run regression $T$ times for given sample size $T$, luckily, for linear models, we can calculate sample CV value by running regression only once. Formally, the leave-one-out cross validation residuals can be computed from the full sample least squares residuals, $\tilde{e_{t}} = \frac{\hat{e_{t}}}{1 - h_{t}}$, where $h_{t} = x_{t}'(X_{t}'X_{t})^{-1}x_{t}$ is the leverage associated with observation $t$, $\hat{e_{t}}$ is the full sample least squares residual and $\tilde{e_{t}}$ is the cross-validation residual. So we can rewrite equation~\ref{cv} as
\begin{equation}
    CV_T(k) = \frac{1}{T}\sum_{t=1}^{T}\left( \frac{\hat{e_{t}}(k)}{1 - h_{t}} \right)^{2}
\end{equation}
In the next section we are going to show how model averaging weights are derived from the cross-validation criterion.