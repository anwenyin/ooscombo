\section{Introduction}
<<<<<<< HEAD
Forecast combination or model averaging has been a useful tool employed by econometricians and industry forecasters in studying many macroeconomic and financial time series, for example, GDP growth rate, unemployment rate, inflation rate and stock market returns. Combination methods such as Granger--Ramanathan, Bates--Granger, Bayesian model averaging, least squares combination, discounted mean square forecast error weights, time--varying combination and survey forecasts combination have been developed for forecasting under various settings.

There are several reasons explaining the popularity of forecast combination or model averaging in empirical research. First, it is highly possible that a single forecasting model is misspecified due to information constraints. For example, predictors that potentially could help boost forecasting performance are not included in the underlying model, so combining forecasts or averaging models may help the forecaster better manage the risk induced in the forecasting process and take advantage of all available information. Even in a stationary world, the true data generating process may be a highly complicated nonlinear function of lags of infinite order and variables which are difficult to measure precisely in practice, consequently, most linear forecasting models proposed by researchers can only be viewed as local approximations for the best linear predictor. It is hard to believe that one predictive model strictly outperforms all other models at all points in time, rather, the best forecasting model may change over time. Due to small sample size for some variables of interest and imperfect information, it is difficult to track the best model based on past forecasting performance. Therefore, combining models can be taken as a practical way to make forecasts robust to misspecification bias, especially when forecasts from various sources are not highly correlated. For example, if the bias is idiosyncratic in each individual model, then combining forecasts from all candidate models may help average out this bias.

Second, a forecasting model's adaptability to parameter instability or structural breaks may not be constant across time. Drastic government policy changes or financial institution reform may bring about structural breaks in the time series variable of interest. An example worth mentioning here is the Great Moderation. Many researchers \cite{Stock_Watson_JEL2003} \cite{Stock_DEECON2004} agree that there is a structural break in the volatility of the U.S. GDP growth rate around mid-1980s as the series becomes less volatile since then. Other developed countries, such as Canada and Germany, have seen the same pattern starting around the same period.\footnote{Arguments explaining this phenomenon include technology progress or innovation, monetary policy change and financial system reform, etc.} Depending on the magnitude and the frequency of the break process, forecasters may prefer a non-stationary model in which all or some of the parameters have changed around the estimated break dates to a stable model where all parameters are assumed constant, but problems arises when the magnitude of the break is small or the evidence of parameter instability is not convincing. In this case, the pre-test model, the single forecasting model selected based on hypothesis testing or information criteria, may not be the best choice for prediction if we assess and compare its performance with other candidates according to mean squared forecast error (\textbf{MSFE}). Why? On one hand, the estimation or dating of structural break can be very imprecise. On the other hand, the quality of the break dates estimates depends not only on the break size measured by some metric, but also on whether the impact of the break is dominated by the volatility of the process\footnote{We have conducted simulation for this case. Our simulation results indicate that, even if there is a break in the conditional mean of the DGP, as long as the magnitude of the break is strictly dominated by the variance of the error term, it turns out that the stable version of the DGP forecasts better than the true DGP evaluated by root mean squared forecast error on average.}. Additionally, for some time series variable of interest, we may reach different conclusions if we study the same variable with different data frequency. For instance, researchers have conducted research on the stock market returns based on various frequency choices, daily, monthly, quarterly or yearly. For the structural break analysis, it is hard to confirm or prove that the estimated structural break dates from all frequencies coincide\footnote{For example, the estimated break date based on monthly data does not fall into the same year if estimated using yearly data. There are several empirical papers \cite{rapach_wohar_JFE2006} \cite{paye_timmermann_JEF2006} related to dating structural breaks based on different data frequencies and models.}. Given this model selection uncertainty, forecast combination may offer diversification gains that make it attractive to average the break and stationary models, rather than relying on a pre-test model. See Timmermann \cite{timmermann2006forecast} for a comprehensive survey of forecast combination.

In an empirical paper studying the U.S. aggregate equity market returns, Rapach, Strauss and Guo \cite{rsz2010} argue that forecast combination is a powerful tool against structural breaks in predicting excess stock returns. For given sample split choices, according to Campbell and Thompson's \cite{campbell_thompson_RFS2008} out-of-sample $R^{2}$ statistic, they show that forecasts generated by pooling all fifteen models are more accurate than those obtained from any single forecasting model or the large kitchen-sink model. But they do not provide detailed econometric theory explaining why forecast combination methods, such as equal weight and discounted mean squared forecast error weight used in their paper, may help deal with structural break. 

With these potential benefits mentioned above, a puzzle associated with forecast combination is that in many empirical applications, equally weighted forecast schemes, i.e., each candidate model receives weight one divided by the total number of models, tend to perform better than various optimal combination weights proposed by researchers, notably the Granger--Ramanathan combination. A paper attempting to explain this puzzle is written by Elliott \cite{elliott11}. Elliott argues that if the variance of the unforecastable component of the variable is large, the gains from optimal forecast combination will be strictly dominated by the unpredictable component. Additionally, the noise introduced by estimating various optimal combination weights, especially when the number of weights is large, further reduces combination gains. 

Having all these benefits and drawbacks mentioned above in mind, in this paper, we focus on the situation where forecasts are generated by two competing models and study if we can come up with model averaging weights possibly superior to others in terms of better managing structural breaks and conditional heteroscedasticity. These two competing models share the same regressors, but one has structural breaks in the conditional mean while the other is stable. This framework applies to situations in which: (i).Researchers or forecasters cannot find convincing evidence supporting structural breaks; (ii).The model is not correctly specified. Our paper adapts Hansen's Mallows model averaging method \cite{hansen2009averaging} to the study of out-of-sample forecasting with breaks. Specifically, we propose model averaging weights derived from the cross--validation information criterion to combine the break model and the stable model. 

The cross--validation information criterion is an unbiased estimate of the mean squared forecast error or the expected test error rate in the language of statistical learning \cite{elements_statistical_learning}, so naturally, it is appropriate to apply CV to the out-of-sample forecasting and forecast evaluation analysis. Studies have shown that the cross--validation criterion outperforms various other criteria in model selection under conditional heteroscedasticity, notably in determining the order of ARMA model. Under the assumption of conditional homoscedasticity, we show that the cross--validation model averaging weights are asymptotically equivalent to Hansen's weights. A natural extension is to relax this homoscedastic error assumption as it may be too strict for many relevant empirical applications. Our main contribution is to derive the cross--validation model averaging weights under conditional heteroscedasticity with breaks, and to show that CV weights are the correct weights minimizing the expected mean squared forecast error in this situation. Monte Carlo evidence and empirical examples are provided to support our results.
=======
Forecast combination or model averaging has been a useful tool employed by econometricians and industry forecasters in studying many macroeconomic and financial time series, for example, GDP growth rate, monthly unemployment rate, inflation rate and stock market returns. Combination methods such as Granger--Ramanathan, Bates--Granger, Bayesian model averaging, least squares combination, discounted mean square forecast error weights, time--varying combination and survey forecasts combination have been developed for forecasting under various settings. 

There are several reasons explaining the popularity of forecast combination or model averaging in empirical research. First, it is highly possible that a single forecasting model is misspecified due to information constraints. For example, predictors that potentially could help boost forecasting performance are not included in the underlying model, so combining forecasts or averaging models may help the forecaster better manage the risk induced in the forecasting process and take advantage of all available information. Even in a stationary world, the true data generating process may be a highly complicated nonlinear function of lags of infinite order and other variables. Most linear forecasting models proposed by researchers are only viewed as local approximations for the best linear predictor, so it is hard to believe that one model strictly dominates all others at all points in time. Rather, the best forecasting model may change over time. Due to small sample size and imperfect information, it is difficult to track the best model based on past forecasting performance. Averaging models can be taken as a way to make forecasts robust to misspecification bias. If the bias is idiosyncratic in each individual model, then combining forecasts from all candidate models may help average out this bias. 

Second, a forecasting model's adaptability to parameter instability or structural breaks may not be constant across time. Drastic government policy changes or financial institution reform may bring about structural breaks in the time series variable of interest. An example worth mentioning here is the Great Moderation. Many researchers \cite{Stock_Watson_JEL2003} \cite{Stock_DEECON2004} agree that there is a structural break in the volatility of the U.S. GDP growth rate around mid-1980s as the series becomes less volatile since then. Other developed countries, such as Canada and Germany, have seen the same pattern starting around the same period. Arguments explaining this phenomenon include technology progress, monetary policy change and financial system reform, etc. Depending on the magnitude and the frequency of the break process, forecasters may prefer a non-stationary model in which all or some of the parameters have changed around the estimated break dates to a stable model where all parameters are assumed constant. Problems arise when the magnitude of the break is small or the evidence of parameter instability is not convincing. In this case, the pre-test model---the single forecasting model selected based on hypothesis testing or information criteria may not be the best choice to generate forecasts in the sense of further reducing mean squared forecast error (\textbf{MSFE}). On one hand, the estimation or dating of structural break can be very imprecise. On the other hand, the quality of the break dates estimates depends not only on the break size measured by some metric, but also on whether the impact of the break is dominated by the volatility of the process\footnote{We have conducted simulation for this case. Our simulation results indicate that, even if there is a break in the conditional mean of the DGP, as long as the magnitude of the break is strictly dominated by the variance of the error term, it turns out that the stable version of the DGP forecasts better than the true DGP in the sense of smaller root mean squared forecast error on average.}. Additionally, even for a selected time series of interest, for example, the aggregate stock market returns, we have several choices of data frequency for consideration (i.e., monthly, quarterly and yearly data). It is hard to confirm or prove that the estimated structural break dates from all frequencies coincide\footnote{For example, the estimated break date based on monthly data falls exactly into the same year if estimated using yearly data. There are several empirical papers \cite{rapach_wohar_JFE2006} \cite{paye_timmermann_JEF2006} related to dating structural breaks based on different data frequencies and models. Although these papers show clear evidence of structural break, but the estimates of break dates are very noisy as different model provides different estimates of dates. Since a forecaster needs to use the upstream model estimation results as input to predict the future, a highly imprecise estimate of the structural break date may have a negative impact on the quality of forecasts.}. Given this model selection uncertainty, forecast combination may offer diversification gains that make it attractive to average the break and stationary models, rather than relying on a pre-test model. See Timmermann \cite{timmermann2006forecast} for a comprehensive survey of forecast combination.

In an empirical paper studying the U.S. aggregate equity market returns, Rapach, Strauss and Guo \cite{rsz2010} argue that forecast combination is a powerful tool against structural breaks in predicting excess stock returns. For given sample split choices, they show that forecasts generated by pooling all fifteen models are more accurate than those obtained from any single forecasting model or the large kitchen-sink model. The quality of forecasts is measured by Campbell and Thompson's \cite{campbell_thompson_RFS2008} out-of-sample $R^{2}$ statistic. But this paper does not provide detailed econometric theory explaining why forecast combination, for example, methods such as equal weight and discounted mean squared forecast error weight used in their paper, may help deal with structural break. Moreover, structural breaks can take various forms, such as discrete or continuous breaks in regression coefficients or in the unconditional variance. Their paper does not address under which type of parameter instability is it suitable to combine forecasts or models.

With these potential benefits mentioned above, a puzzle associated with forecast combination is that in many empirical applications, equally weighted forecast schemes, i.e., each candidate model receives weight one divided by the total number of models, tend to perform better than various optimal combination weights proposed by researchers, notably the Granger--Ramanathan combination. A paper attempting to explain this puzzle is written by Elliott \cite{elliott11}. Elliott argues that if the variance of the unforecastable component of the variable is large, the gains from optimal forecast combination will be strictly dominated by the unpredictable component. Additionally, the noise introduced by estimating various optimal combination weights, especially when the number of weights is large, further reduces combination gains. Another drawback of forecast combination is that in many empirical applications, researchers or industry practitioners tend to ignore the econometric models associated with these forecasts. In return, this may result in omitting potentially important information related to combination weights.

Having all these benefits and drawbacks mentioned above in mind, in this paper, we focus on the situation where forecasts are generated by two competing models and study if we can come up with model averaging weights possibly superior to others in terms of better managing structural breaks and conditional heteroscedasticity. These two models share the same regressors, but one has structural breaks in the regression coefficients while the other is stable. This framework applies to situations in which: (i).researchers or forecasters cannot find convincing evidence supporting structural breaks; (ii).breaks are of small magnitude, or (iii).the model is not correctly specified. Our paper adapts Hansen's Mallows model averaging method \cite{hansen2009averaging} to the study of out-of-sample forecasting with breaks. Specifically, we propose model averaging weights derived from the cross--validation information criterion combining the break model and the stable model. Cross--validation (\textbf{CV}) criterion is an unbiased estimate of the mean squared forecast error, so intuitively, it justifies the use of CV weights to forecast out-of-sample. Under the assumption of conditional homoscedasticity, we show that the cross--validation model averaging weights are approximately identical to Mallows' weights proposed by Hansen. A natural extension is to relax this homoscedastic error assumption as it may not be relevant to many empirical applications. Studies have shown that the cross--validation criterion performs better than various rival criteria in model selection under conditional heteroscedasticity, especially in determining the order of ARMA model. Our major contribution is to derive the cross--validation model averaging weights under conditional heteroscedasticity, and to show that CV weights are the correct weights minimizing the population mean squared forecast error in this situation. Monte Carlo evidence and empirical examples are provided to support our results.
>>>>>>> 2966618e04912eba46807909c13546d2481a77e9

The remainder of this paper is organized as follows: section 2 provides related literature review. Section 3 first describes the econometric model and the forecasting problem, then presents theoretical results for the model averaging weights. Section 4 presents Monte Carlo evidence. Section 5 provides two empirical examples comparing our method with others. Section 6 concludes. 