\appendix
\section{Proof}

\begin{proof}[Proof of Proposition~\ref{thm:1}]
From the cross-validation criterion, for linear models we have the well-known result that
    \begin{equation*}
    \frac{1}{T} \sum_{i=1}^T \tilde{e_t}^2 = \frac{1}{T} \sum_{i=1}^T \frac{\tilde{e_t}^2}{(1-h_t)^2}
    \end{equation*}
where $h_t = x_t'(X'X)^{-1}x_t$ is the leverage associated with observation $t$. Applying Taylor expansion, we can expand the above equation as
    \begin{eqnarray*}
    \frac{1}{T} \sum_{i=1}^T \tilde{e_t}^2 & = & \frac{1}{T} \sum_{i=1}^T \frac{\tilde{e_t}^2}{(1-h_t)^2} \\
                                           & \approx & \frac{1}{T} \sum_{i=1}^T \hat{e_t}^2 + \frac{2}{T} \sum_{i=1}^T \hat{e_t}^2 h_t \\
                                           & = & \hat{\sigma}^2 + \frac{2}{T} \sum_{i=1}^T \hat{e_t}^2 x_t' (X'X)^{-1} x_t
    \end{eqnarray*}
Under regularity conditions listed in Assumption~\ref{asump:1}, we have $\hat{\sigma}^2 \stackrel{p}{\rightarrow} \sigma^{2}$, and for the penalty term,  $\frac{1}{T} \sum_{i=1}^T \hat{e_t}^2 x_t' (X'X)^{-1} x_t \stackrel{p}{\rightarrow} E(\boldmath{e'Pe})$, putting these two parts together, we can see that CV is asymptotically equivalent to Mallows' Cp under our assumptions except for conditionally homoscedastic errors.
\end{proof}
<<<<<<< HEAD

\begin{proof}[Proof of Corollary~\ref{corollary:1}]
Since CV is asymptotically equivalent to Mallows' Cp, following Hansen's \cite{hansen2009averaging} proof, write the sample CV criterion for the weighted model as a function of the break model weight $w$,
    \begin{equation*}
      \mathrm{CV}(w) = (w\hat{e} + (1-w)\tilde{e})'(w\hat{e} + (1-w)\tilde{e}) + 2(T - 2k)^{-1}(k + w\bar{p})\hat{e}'\hat{e}
    \end{equation*}
where $\bar{p}$ proposed by Hansen is used to approximate the infeasible expected value of the population penalty term. The CV weight is the value in $[0, 1]$ that minimizes $\mathrm{CV}(w)$, so
    \begin{equation*}
      \hat{w} = \frac{(T - 2k)(\sum_{t=1}^{T}\tilde{e}_{t}^{2} - \sum_{t=1}^{T}\hat{e}_{t}^{2}) - \bar{p}\sum_{t=1}^{T}\hat{e}_{t}^{2}}{(T - 2k)(\sum_{t=1}^{T}\tilde{e}_{t}^{2} - \sum_{t=1}^{T}\hat{e}_{t}^{2})}
    \end{equation*}
if $(T - 2k)(\sum_{t=1}^{T}\tilde{e}_{t}^{2} - \sum_{t=1}^{T}\hat{e}_{t}^{2})(\sum_{t=1}^{T}\hat{e}_{t}^{2})^{-1} \geq \bar{p}$ while $\hat{w} = 0$ otherwise.
\end{proof}
=======
\begin{proof}[Proof of Lemma~\ref{thm:2}]
We relax the conditional homoscedasticity assumption in Hansen's Mallow's model averaging method when there is a possible structural break in the underlying DGP. The proof is adapted from Hansen \cite{hansen2009averaging} and Andrews \cite{andrews93} without assuming conditional homoscedasticity. The CV penalty term can be expanded as:
\begin{eqnarray*}
e'P(k)e & = & e'Pe + e'P^{*}(k)e \\
        & = & e'Pe + e'X^{*}(k)(X^{*}(k)'X^{*}(k))^{-1}X^{*}(k)'e
\end{eqnarray*}
where the notations for $P$,$P^{*}(k)$ and $X^{*}(k)$ are the same as in Hansen \cite{hansen2009averaging}.

For the second term, $e'X^{*}(k)(X^{*}(k)'X^{*}(k))^{-1}X^{*}(k)'e$, we can see
\begin{eqnarray*}
X^{*}(k)'X^{*}(k) & = & (X(k)-X(X'X)^{-1}X(k)'X(k))'(X(k)-X(X'X)^{-1}X(k)'X(k)) \\
                  & = & X(k)'X(k) - X(k)'X(X'X)^{-1}X(k)'X(k) \\
				  &   & - X(k)'X(k)(X'X)^{-1}X'X(k) + X(k)'X(k)(X'X)^{-1}X(k)'X(k) \\
				  & \stackrel{P}{\rightarrow} & \pi Q - \pi QQ^{-1} \pi Q - \pi QQ^{-1} \pi Q + \pi QQ^{-1} \pi Q \\
				  & = & \pi (1-\pi)Q
\end{eqnarray*}

By continuous mapping theorem we have $(X^{*}(k)'X^{*}(k))^{-1}\stackrel{P}{\rightarrow}(\pi (1-\pi))^{-1}Q^{-1}$.
>>>>>>> 2966618e04912eba46807909c13546d2481a77e9

\begin{proof}[Proof of Proposition~\ref{thm:2}]
The proof of this proposition is adapted from Hansen \cite{hansen2009averaging}. By projection arguments, $P(m) = P + P^{*}(m)$, where $P = X(X'X)^{-1}X'$, $P^{*}(m) = X^{*}(m)(X^{*}(m)'X^{*}(m))^{-1}X^{*}(m)'$, $X^{*}(m) = X(m) - PX(m) = X(m) - X(X'X)^{-1}X'X(m) = X(m) - X(X'X)^{-1}X(m)'X(m)$, and $X(m)$ is the matrix of stacked regressors $x_{t}(t < m)$, the cross-validation penalty term can be expanded as:
    \begin{eqnarray*}
    e'P(m)e & = & e'Pe + e'P^{*}(m)e \\
            & = & e'Pe + e'X^{*}(m)(X^{*}(m)'X^{*}(m))^{-1}X^{*}(m)'e
    \end{eqnarray*}

\noindent We start by showing the asymptotic distribution of the second term on the right-hand-side of the above equation, $e'P^{*}(m)e = e'X^{*}(m)(X^{*}(m)'X^{*}(m))^{-1}X^{*}(m)'e$. For the meat part, $X^{*}(m)'X^{*}(m)$, we have
    \begin{eqnarray*}
    X^{*}(m)'X^{*}(m) & = & (X(m)-X(X'X)^{-1}X(m)'X(m))'(X(m)-X(X'X)^{-1}X(m)'X(m)) \\
                      & = & X(m)'X(m) - X(m)'X(X'X)^{-1}X(m)'X(m) \\
    				  &   & \quad - X(m)'X(m)(X'X)^{-1}X'X(m) \\
                      &   & \quad + X(m)'X(m)(X'X)^{-1}X(m)'X(m) \\
    				  & = & X(m)'X(m) - X(m)'X(X'X)^{-1}X(m)'X(m)
    \end{eqnarray*}
\noindent From our assumptions and $\frac{m}{T} \rightarrow \pi$, by laws of large numbers, we have
    \begin{equation*}
        \frac{1}{T} X(m)'X(m) \stackrel{P}{\rightarrow} \pi Q
    \end{equation*}
and
    \begin{equation*}
        \frac{1}{T} X(m)'X(X'X)^{-1}X(m)'X(m) \stackrel{P}{\rightarrow} \pi QQ^{-1} \pi Q
    \end{equation*}
so 
    \begin{equation*}
        \frac{1}{T} X^{*}(m)'X^{*}(m) \stackrel{P}{\rightarrow} \pi (1-\pi)Q
    \end{equation*}

\noindent By continuous mapping theorem we have 
    \begin{equation*}
        (\frac{1}{T} X^{*}(m)'X^{*}(m))^{-1}\stackrel{P}{\rightarrow}(\pi (1-\pi))^{-1}Q^{-1}
    \end{equation*}
\noindent For the bread part, $X^{*}(m)'e = X(m) - X(X'X)^{-1}X(m)'X(m))'e$, we can show 
    \begin{eqnarray*}
         X(m) - X(X'X)^{-1}X(m)'X(m))'e & = & X(m)'e - X(m)'X(m)(X'X)^{-1}X'e \\
                                        & = & \jia x_t e_t - \jia x_t x_t' \left( \jian x_t x_t' \right) ^{-1} \left( \jian x_t e_t \right)
    \end{eqnarray*}

\noindent Next, applying laws of large numbers and the mixing functional central limit theorem, we have
    \begin{equation*}
        \frac{1}{\sqrt{T}} \jia x_t e_t \Rightarrow W(\pi)
    \end{equation*}
        
    \begin{equation*}
        \frac{1}{T} \jia x_t x_t' \stackrel{P}{\rightarrow} \pi Q
    \end{equation*}
    
    \begin{equation*}
        \left( \frac{1}{T} \jian x_t x_t' \right) ^{-1} \stackrel{P}{\rightarrow} Q
    \end{equation*}
    
    \begin{equation*}
        \frac{1}{\sqrt{T}} \jian x_t e_t \Rightarrow W(1)
    \end{equation*}
where $W(1)$ is the Brownian motion vector with covariance matrix $\Sigma \equiv \lim\limits_{n\to\infty}$VAR$(\rn\jian X_i e_i)$, and $W(\pi)$ is the Brownian vector at time $\pi$.

<<<<<<< HEAD
\noindent Putting together results obtained above, we have
    \begin{equation*}
        \frac{1}{\sqrt{T}} X^{*}(m)'e \Rightarrow W(\pi) - \pi W(1)
    \end{equation*}
=======
\begin{eqnarray*}
e'P^{*}(k)e & = & e'X^{*}(k)(X^{*}(k)'X^{*}(k))^{-1}X^{*}(k)'e \\
            & \stackrel{P}{\rightarrow} & \frac{1}{\pi(1-\pi)} (W(\pi) - \pi W(1))'Q^{-1}(W(\pi) - \pi W(1)) =  \frac{\mathbf{B}(\pi)'\mathbf{B}(\pi)}{\pi(1-\pi)}\equiv J_0(\xi_{\delta})
\end{eqnarray*}
where $\mathbf{B}(\pi)$ is a Brownian bridge. Combined with Hansen's \cite{hansen2009averaging} theorem 1 without assuming conditional homoscedasticity or Andrews' \cite{andrews93} theorem 4, our results follow.
>>>>>>> 2966618e04912eba46807909c13546d2481a77e9

\noindent Then we have
    \begin{equation*}
        \frac{1}{T} e'P^{*}(m)e \Rightarrow \frac{1}{\pi(1-\pi)} (W(\pi) - \pi W(1))'Q^{-1}(W(\pi) - \pi W(1)) =  \frac{\mathbf{B}(\pi)'\mathbf{B}(\pi)}{\pi(1-\pi)}
    \end{equation*}
where $\mathbf{B}(\pi)$ is a Brownian bridge. Combined with Hansen's \cite{hansen2009averaging} theorem 1 without assuming conditional homoscedasticity or Andrews' \cite{andrews93} theorem 4, we have $\frac{1}{T} e'P^{*}(m)e \Rightarrow J_0(\xi_{\delta})$.

\noindent For the first component in the penalty term, $e'Pe$, we have
    \begin{equation*}
      e'Pe = (\frac{1}{\sqrt{T}} \jian x_t e_t)'(\frac{1}{T} \jian x_t x_t')^{-1}(\frac{1}{\sqrt{T}} \jian x_t e_t)
    \end{equation*}
\noindent Again, applying relevant laws of large numbers and central limit theorem, 
    \begin{equation*}
      \frac{1}{\sqrt{T}} \jian x_t e_t \Rightarrow W(1)
    \end{equation*}

<<<<<<< HEAD
    \begin{equation*}
      \frac{1}{T} \jian x_t x_t' \stackrel{p}{\rightarrow} Q
    \end{equation*}
so
    \begin{equation*}
      e'Pe \stackrel{p}{\rightarrow} \Xi' Q^{-1} \Xi
    \end{equation*}
where $\Xi \sim N(0, \Sigma)$ .
=======
Under the assumption of conditional homoscedasticity, the above distribution is a $\chi^2$ distribution times the variance of the population error. If we relax the assumption of conditional homoscedasticity, $e'Pe$ does not converge to a $\chi^2$ distribution, instead, it converges to a weighted sum of $\chi^2$ distribution of one degree of freedom.
>>>>>>> 2966618e04912eba46807909c13546d2481a77e9

\noindent $\Sigma$ is symmetric and positive definite, $Q^{-1}$ is of the same rank of $\Sigma$, applying results of the distribution of quadratic forms (see section 5.4 of Ravishanker and Dipak \cite{linear_model_textbook}), we have
    \begin{equation*}
      e'Pe \stackrel{d}{\rightarrow} \sum_{j=1}^{k} \lambda_j \chi^2(1)
    \end{equation*}
where $\lambda_j$s are the eigenvalues of the matrix $Q^{-1}\Sigma$, $\chi^2(1)$ is a random variable having the $\chi^2$ distribution with degree of freedom one. 

<<<<<<< HEAD
\noindent Collecting all results shown above, we have
    \begin{equation*}
      e'P(\hat{m})e \stackrel{d}{\rightarrow} \sum_{j=1}^{k} \lambda_j \chi^2(1) + J_0(\xi_{\delta})
    \end{equation*}
\end{proof}

\begin{proof}[Proof of Corollary~\ref{corollary:2}]
From proposition~\ref{thm:2}, take expectation of the CV penalty term,
    \begin{equation*}
      E(e'P(\hat{m})e) = E(\sum_{j=1}^{k} \lambda_j \chi^2(1)) + E(J_0(\xi_{\delta}))
    \end{equation*}
we have $E(\sum_{j=1}^{k} \lambda_j \chi^2(1)) = \sum_{j=1}^{k} \lambda_j$, applying Hansen's technique, approximate the value of $E(J_0(\xi_{\delta}))$ by averaging two extreme cases, so $E(J_0(\xi_{\delta})) \approx \frac{1}{2}(\mathrm{tr}(\hat{Q}^{-1}\hat{\Sigma}) + 2\bar{p} - k) \equiv \bar{p}^{*}$. Then by the same procedure in the proof of corollary~\ref{corollary:2},
    \begin{equation*}
      \mathrm{CV}(w) = (w\hat{e} + (1-w)\tilde{e})'(w\hat{e} + (1-w)\tilde{e}) + 2(\mathrm{tr}(\hat{Q}^{-1}\hat{\Sigma}) + w \bar{p}^{*})
    \end{equation*}
\noindent The CV weight is the value in $[0, 1]$ that minimizes $\mathrm{CV}(w)$, so
    \begin{equation*}
      \hat{w} = 1 - \frac{\mathrm{tr}\left(\hat{Q}^{-1}\hat{\Sigma}\right) + 2\bar{p} - k}{2\left(\sum_{t=1}^{T}\tilde{e}_t^2 - \sum_{t=1}^{T}\hat{e}_t^2\right)}
    \end{equation*}
if $(\sum_{t=1}^{T}\tilde{e}_{t}^{2} - \sum_{t=1}^{T}\hat{e}_{t}^{2}) \geq \bar{p}^{*}$ while $\hat{w} = 0$ otherwise.
=======
\begin{eqnarray*}
e'Pe & \stackrel{d}{\rightarrow} & \sum_{j=1}^{k} \lambda_j \chi^2(1)
\end{eqnarray*}
where $\lambda_j$s are the eigenvalues of the matrix $L'Q^{-1}L$, $\Sigma = LL'$. Although the asymptotic distribution is not pivotal, since we are only interested in the first moment of this distribution, we can easily obtain its value by $\mathrm{E}(e'Pe) = \sum_{j=1}^{k} \lambda_j = \tr{(L'Q^{-1}L)} = \tr{(Q^{-1} \Sigma)}$, where $Q$ and $\Sigma$ can be estimated by their sample analogues. This can be done without relying on potentially time-consuming Monte Carlo simulation.
\end{proof}
\begin{proof}[Proof of Theorem~\ref{lem:2}]
The proof of this theorem follows Hansen\cite{hansen2009averaging}, just replace all relevant terms according to Theorem~\ref{thm:2}.
>>>>>>> 2966618e04912eba46807909c13546d2481a77e9
\end{proof}
